{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1: The splitted sentences from the text are:  ['The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old.', 'Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.']\n",
      "Exercise 2: The text with the replaced words is:  The president of the USA, Donald Trump, is 190 centimeters high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.\n",
      "Exercise 3: The text with the replaced words is:  the president of the USA, Donald_Trump, is 190 centimeters high and 78 years old. Forbes_Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-february 2025.\n",
      "Exercise 4: The splitted words from the text are:  ['the', 'president', 'of', 'the', 'USA', ',', 'Donald_Trump', ',', 'is', '190', 'centimeters', 'high', 'and', '78', 'years', 'old', '.', 'Forbes_Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', 'five', 'point', 'five', 'billion', 'as', 'of', 'mid-february', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "# For the next text, perform the following actions\n",
    "text = \"The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.\"\n",
    "\n",
    "# (1 point) 1 - Use NLTK to split the sentences \n",
    "\n",
    "import nltk #we import the nltk library\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #we import functions seen in class\n",
    "splitted_sentences = sent_tokenize(text) #we tokenize the text into sentences\n",
    "print(\"Exercise 1: The splitted sentences from the text are: \", splitted_sentences) #we print tokenized sentences\n",
    "\n",
    "# (2 points) 2 - Convert with regex the acronym U.S.A. to USA, the number 1.9m to 190 centimeters or any other number of a height like that (e.g. 1.75m to 175 centimeters), and \"$5.5 billion\" to five point five billion. \n",
    "\n",
    "import re # we import the regex library\n",
    "\n",
    "text = re.sub(r\"U.S.A.\", \"USA\", text) #we substitute the acronym U.S.A. with USA\n",
    "text = re.sub(r\"1.9m\", \"190 centimeters\", text) #we substitute the number 1.9m with 190 centimeters\n",
    "text = re.sub(r\"\\$5.5 billion\", \"five point five billion\", text) #we substitute the number $5.5 billion with five point five billion\n",
    "print(\"Exercise 2: The text with the replaced words is: \", text) #we print the text with the replaced words\n",
    "# If i write it all in one line, it doesn't work (It creates three different sentences), so i do it one by one\n",
    "\n",
    "# (1 point) 3 - Convert to lowercase except the proper nouns that must keep the original case. For the multiword proper names convert them to an unique word joining the two word with underscoere (Juan Fernández -> Juan_Fernández).\n",
    "\n",
    "text = re.sub(r\"The\", \"the\", text) #we substitute the word The with the word the\n",
    "text = re.sub(r\"February\", \"february\", text) #we substitute the word February with the word february\n",
    "text = re.sub(r\"Donald Trump\", \"Donald_Trump\", text) #we substitute the name Donald Trump with Donald_Trump\n",
    "text = re.sub(r\"Forbes Magazine\", \"Forbes_Magazine\", text) #we substitute the name Forbes Magazine with Forbes_Magazine\n",
    "print(\"Exercise 3: The text with the replaced words is: \", text) #we print the text with the replaced words\n",
    "\n",
    "# (1 point) 4 - Tokenize the text (use the tool you prefer). \n",
    "\n",
    "from nltk.tokenize import word_tokenize #we import the word_tokenize function from the nltk library\n",
    "#We'll do it as we have seen in class (S04_2_explanation.ipynb)\n",
    "splitted_words = word_tokenize(text) #we create a list to contain the tokenized words\n",
    "print(\"Exercise 4: The splitted words from the text are: \", splitted_words) #we print tokenized words\n",
    "\n",
    "# (1 point) 5 - Remove the stopwords (use the tool you prefer).\n",
    "\n",
    "from nltk.corpus import stopwords #we import the stopwords from the nltk library as in class (S04_3_exercises_after.ipynb)\n",
    "stop_words = set(stopwords.words(\"english\")) #we create a set with the stopwords in english\n",
    "#we delete the stopwords from the text\n",
    "\n",
    "\n",
    "# (1 point) 6 - Create bigrams with pure python.\n",
    "\n",
    "# (2 point) 7 - Create a language model that predict the next word using bigrams. Please explain in the code how you made the calculations.\n",
    "\n",
    "\n",
    "#https://github.com/carlos89p/NLP_portfolio\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
