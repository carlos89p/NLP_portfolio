{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_hosDAcZwSq"
   },
   "source": [
    "## Bag of N-Grams\n",
    "\n",
    "##### One hot encoding, BoW and TF-IDF treat words as independent units. There is no notion of phrases or word ordering. Bag of Ngrams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n countigous words/tokens. This can help us capture some context, which earlier approaches could not do. Let us see how it works using the same toy corpus we used in earlier examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.21.3\n",
      "  Using cached scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages (from scikit-learn==0.21.3) (2.2.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages (from scikit-learn==0.21.3) (1.15.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages (from scikit-learn==0.21.3) (1.3.2)\n",
      "Building wheels for collected packages: scikit-learn\n",
      "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[69 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/setup.py:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  \u001b[31m   \u001b[0m   from pkg_resources import parse_version\n",
      "  \u001b[31m   \u001b[0m Partial import of sklearn during the build process.\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/setup.py:122: DeprecationWarning:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "  \u001b[31m   \u001b[0m   of the deprecation of `distutils` itself. It will be removed for\n",
      "  \u001b[31m   \u001b[0m   Python >= 3.12. For older Python versions it will remain present.\n",
      "  \u001b[31m   \u001b[0m   It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "  \u001b[31m   \u001b[0m   For more details, see:\n",
      "  \u001b[31m   \u001b[0m     https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   from numpy.distutils.command.build_ext import build_ext  # noqa\n",
      "  \u001b[31m   \u001b[0m INFO: C compiler: clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/carlosillanaldariz/miniconda3/include -arch arm64 -fPIC -O2 -isystem /Users/carlosillanaldariz/miniconda3/include -arch arm64\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m INFO: compile options: '-c'\n",
      "  \u001b[31m   \u001b[0m extra options: '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m INFO: clang: test_openmp.c\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/setup.py\", line 290, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup_package()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/setup.py\", line 286, in setup_package\n",
      "  \u001b[31m   \u001b[0m     setup(**metadata)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/numpy/distutils/core.py\", line 135, in setup\n",
      "  \u001b[31m   \u001b[0m     config = configuration()\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/setup.py\", line 174, in configuration\n",
      "  \u001b[31m   \u001b[0m     config.add_subpackage('sklearn')\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/numpy/distutils/misc_util.py\", line 1041, in add_subpackage\n",
      "  \u001b[31m   \u001b[0m     config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
      "  \u001b[31m   \u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/numpy/distutils/misc_util.py\", line 1007, in get_subpackage\n",
      "  \u001b[31m   \u001b[0m     config = self._get_configuration_from_setup_py(\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/numpy/distutils/misc_util.py\", line 949, in _get_configuration_from_setup_py\n",
      "  \u001b[31m   \u001b[0m     config = setup_module.configuration(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/sklearn/setup.py\", line 76, in configuration\n",
      "  \u001b[31m   \u001b[0m     maybe_cythonize_extensions(top_path, config)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/sklearn/_build_utils/__init__.py\", line 42, in maybe_cythonize_extensions\n",
      "  \u001b[31m   \u001b[0m     with_openmp = check_openmp_support()\n",
      "  \u001b[31m   \u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x9/s464tr3n2ng8_s8c_5cxv5800000gn/T/pip-install-pnc31ryn/scikit-learn_fecbf72f9261480f9daefd73a065796b/sklearn/_build_utils/openmp_helpers.py\", line 140, in check_openmp_support\n",
      "  \u001b[31m   \u001b[0m     raise CompileError(err_message)\n",
      "  \u001b[31m   \u001b[0m distutils.errors.CompileError:\n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m It seems that scikit-learn cannot be built with OpenMP support.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - Make sure you have followed the installation instructions:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://scikit-learn.org/dev/developers/advanced_installation.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If your compiler supports OpenMP but the build still fails, please\n",
      "  \u001b[31m   \u001b[0m   submit a bug report at:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://github.com/scikit-learn/scikit-learn/issues\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If you want to build scikit-learn without OpenMP support, you can set\n",
      "  \u001b[31m   \u001b[0m   the environment variable SKLEARN_NO_OPENMP and rerun the build\n",
      "  \u001b[31m   \u001b[0m   command. Note however that some estimators will run in sequential\n",
      "  \u001b[31m   \u001b[0m   mode and their `n_jobs` parameter will have no effect anymore.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
      "Failed to build scikit-learn\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-learn)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "!pip install scikit-learn==0.21.3\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch3/ch3-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch3-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wE4F2qCx7NvL",
    "outputId": "1021a10b-9239-4cab-afe5-c08f8910bcef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our corpus\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khNajktSVfnS"
   },
   "source": [
    "##### CountVectorizer, which we used for BoW, can be used for getting a Bag of N-grams representation as well, using its ngram_range argument. The code snippet below shows how: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "j7-VReVY5jZr",
    "outputId": "d4dfff0a-b6e4-4488-f6c7-525722e2fb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNG5mUeWVohV"
   },
   "source": [
    "##### Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bag of N-Grams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
